# -*- coding: utf-8 -*-
"""ì§€í•˜ìˆ˜_ë°ì´í„°.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MvZ1SPWxpjNsMflogZF9f70VvY1Ob3lF
"""

import pandas as pd

temp = pd.read_excel("/content/á„‰á…µá„€á…®á†«á„€á…®á„‡á…§á†¯_á„€á…¡á„†á…®á†·á„ƒá…©á„ƒá…¦á„‹á…µá„á…¥.csv")

temp = temp.groupby(["Unnamed: 1", "Unnamed: 3"], as_index=False)[["SIGUNGU_CD"]].mean()

temp["SIGUNGU_CD"] = temp["2024ë…„6ì›” í–‰ì •êµ¬ì—­ ì½”ë“œ"].astype(str) + temp["Unnamed: 2"].astype(str)

temp["SIGUNGU_CD"] = temp["SIGUNGU_CD"].astype(int)

temp

temp.columns = ["ì‹œë„", "ì‹œêµ°êµ¬", "SIGUNGU_CD"]

temp.to_csv("ì‹œêµ°êµ¬_ì½”ë“œ.csv")

"""#Total"""

import pandas as pd

df = pd.read_csv("/content/á„‰á…µá„€á…®á†«á„€á…®á„‡á…§á†¯_á„€á…¡á„†á…®á†·á„ƒá…©á„ƒá…¦á„‹á…µá„á…¥.csv")

# 'Unnamed'ë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë“  ì—´ ì œê±°
df = df.loc[:, ~df.columns.str.startswith('Unnamed')]

df

feature_cols = [ 'ëŒ€ì‘ëŠ¥ë ¥ ê³„ìˆ˜(ë‹¨ìœ„: ì ìˆ˜)', 'ë…¸ì¶œë„ ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)', 'ë¯¼ê°ë„ ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)',
       'ë³´ì¡°ìˆ˜ì› ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)', 'ì·¨ì•½ì„±(ë‹¨ìœ„: ì ìˆ˜)']

"""# nnPU"""

import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# 1. ë°ì´í„° ì¤€ë¹„
X = df[feature_cols].values  # ì‚¬ìš©í•  feature ì»¬ëŸ¼
y = df["ì‚¬ì—…ì—¬ë¶€"].fillna(0).values  # 1: Positive, 0: Unlabeled

scaler = StandardScaler()
X = scaler.fit_transform(X)

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y)

X_train = torch.FloatTensor(X_train)
y_train = torch.FloatTensor(y_train)
X_val = torch.FloatTensor(X_val)
y_val = torch.FloatTensor(y_val)

class SimpleNN(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.linear = nn.Sequential(
            nn.Linear(input_dim, 16),
            nn.ReLU(),
            nn.Linear(16, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.linear(x)

def nnPU_loss(y_pred, y_true, pi=0.1, beta=0):
    positive = y_true == 1
    unlabeled = y_true == 0

    # Positive risk
    pos_pred = y_pred[positive]
    pos_loss = torch.mean(-torch.log(pos_pred + 1e-8))

    # Unlabeled risk
    unl_pred = y_pred[unlabeled]
    neg_loss = torch.mean(-torch.log(1 - unl_pred + 1e-8))

    # PU risk with non-negative constraint
    pu_loss = pi * pos_loss - pi * neg_loss
    return torch.maximum(pu_loss, torch.tensor(beta))

model = SimpleNN(X_train.shape[1])
optimizer = optim.Adam(model.parameters(), lr=0.01)

for epoch in range(100):
    model.train()
    optimizer.zero_grad()
    y_pred = model(X_train).squeeze()
    loss = nnPU_loss(y_pred, y_train, pi=y_train.mean().item())
    loss.backward()
    optimizer.step()

    if epoch % 10 == 0:
        print(f"Epoch {epoch}, Loss: {loss.item():.4f}")

# dfì™€ ë™ì¼í•œ ì¸ë±ìŠ¤ë¥¼ ê°€ì§„ df_valì„ ë”°ë¡œ êµ¬ì„±í–ˆì–´ì•¼ í•¨
X_train, X_val, y_train, y_val, df_train, df_val = train_test_split(
    X_scaled, y, df, test_size=0.2, stratify=y, random_state=42
)

# ì˜ˆì¸¡
model.eval()
with torch.no_grad():
    probs = model(torch.FloatTensor(X_val)).squeeze().numpy()

# ê²°ê³¼ ì €ì¥
df_val = df_val.copy()  # ë³µì‚¬í•´ì„œ ì•ˆì „í•˜ê²Œ
df_val["ì‚¬ì—…í™•ë¥ "] = probs

# ìƒìœ„ 20ê°œ ë³´ê¸°
top_candidates = df_val.sort_values("ì‚¬ì—…í™•ë¥ ", ascending=False).head(20)

import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import seaborn as sns


from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import seaborn as sns
import matplotlib.pyplot as plt

# ì˜ˆì¸¡ ë¼ë²¨ (threshold 0.5 ê¸°ì¤€)
y_pred = (probs > 0.5).astype(int)

# í˜¼ë™í–‰ë ¬
cm = confusion_matrix(y_val, y_pred)
plt.figure(figsize=(4, 3))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

# ë¶„ë¥˜ ë¦¬í¬íŠ¸
print(classification_report(y_val, y_pred))

# ROC-AUC
roc_auc = roc_auc_score(y_val, probs)
print(f"ROC-AUC Score: {roc_auc:.4f}")

sns.histplot(probs, bins=20, kde=True)
plt.title("ì‚¬ì—… í™•ë¥  ë¶„í¬")  # í•œê¸€ë„ ê¹¨ì§€ì§€ ì•ŠìŒ
plt.xlabel("ì‚¬ì—… í™•ë¥ ")
plt.ylabel("ê±´ìˆ˜")
plt.show()

"""# Logistic"""

# ğŸ“Œ 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.preprocessing import StandardScaler


df = merged.copy()

# ğŸ“Œ 3. Feature, Target ì„¤ì •
feature_cols = [
    "ëŒ€ì‘ëŠ¥ë ¥ ê³„ìˆ˜(ë‹¨ìœ„: ì ìˆ˜)",
    "ë…¸ì¶œë„ ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)",
    "ë¯¼ê°ë„ ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)",
    "ë³´ì¡°ìˆ˜ì› ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)",
    "ì·¨ì•½ì„±(ë‹¨ìœ„: ì ìˆ˜)"
]
X = df[feature_cols]
y = df["ì‚¬ì—…ì—¬ë¶€"].fillna(0).astype(int)

# ğŸ“Œ 4. ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ğŸ“Œ 5. í›ˆë ¨/ê²€ì¦ ë¶„ë¦¬
X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, stratify=y, test_size=0.2, random_state=42)

# ğŸ“Œ 6. ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ í•™ìŠµ
lr = LogisticRegression(max_iter=1000, random_state=42)
lr.fit(X_train, y_train)

# ğŸ“Œ 7. ì˜ˆì¸¡ ë° í‰ê°€
y_pred = lr.predict(X_val)
y_prob = lr.predict_proba(X_val)[:, 1]

print("ğŸ“Œ ROC AUC:", roc_auc_score(y_val, y_prob))
print("ğŸ“Œ Classification Report:\n", classification_report(y_val, y_pred))

# ğŸ“Œ 8. í˜¼ë™í–‰ë ¬ ì‹œê°í™”
plt.figure(figsize=(4, 3))
sns.heatmap(confusion_matrix(y_val, y_pred), annot=True, fmt='d', cmap='Greens')
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix (Logistic Regression)")
plt.tight_layout()
plt.show()

# ğŸ“Œ 9. ì „ì²´ ë°ì´í„° ì‚¬ì—… í™•ë¥  ì˜ˆì¸¡
df["ì‚¬ì—…í™•ë¥ "] = lr.predict_proba(X_scaled)[:, 1]

# ğŸ“Œ 10. ì‚¬ì—…ì´ ì•„ì§ ì•ˆ ëœ ê³³ ì¤‘ í™•ë¥  ë†’ì€ ìƒìœ„ 20ê°œ ì¶”ì¶œ
top20 = df[df["ì‚¬ì—…ì—¬ë¶€"] == 0].sort_values("ì‚¬ì—…í™•ë¥ ", ascending=False)

# ğŸ“Œ 11. ì‹œê°í™”
plt.figure(figsize=(8, 6))
sns.barplot(data=top20, y="ì‹œêµ°êµ¬", x="ì‚¬ì—…í™•ë¥ ", palette="crest")
plt.title("ì‚¬ì—… ìœ ë ¥ í›„ë³´ ìƒìœ„ 20ê°œ ì§€ì—­ (ë¡œì§€ìŠ¤í‹±)")
plt.xlabel("ì‚¬ì—… í™•ë¥ ")
plt.ylabel("ì‹œêµ°êµ¬")
plt.tight_layout()
plt.show()

# ğŸ“Œ 12. ìƒìœ„ 20ê°œ ì¶œë ¥
top20

import geopandas as gpd

top20 = gpd.GeoDataFrame(top20, geometry=top20["geometry"])

top20



import geopandas as gpd
import pandas as pd
import matplotlib.pyplot as plt
plt.rcParams['font.family'] = 'AppleGothic'  # ë§¥ ê¸°ë³¸ í•œê¸€ í°íŠ¸
plt.rcParams['axes.unicode_minus'] = False



# ì§€ë„ ì‹œê°í™”
fig, ax = plt.subplots(figsize=(18, 24), facecolor='white',edgecolor='none')  # ì „ì²´ ë°°ê²½ ê²€ì •

# ë°°ê²½ ì „ì²´ íšŒìƒ‰ ë˜ëŠ” í°ìƒ‰ìœ¼ë¡œ ì¹ í•˜ì§€ ì•ŠìŒ
gdf.plot(ax=ax, color='whitesmoke', edgecolor='none', linewidth=0.3)

# anomaly_scoreê°€ ìˆëŠ” ì‹œêµ°êµ¬ë§Œ ìƒ‰ì¹ 
top20.plot(
    column='ì‚¬ì—…í™•ë¥ ',
    ax=ax,
    edgecolor='none',   # ê²½ê³„ì„  ì—†ìŒ
    cmap='YlOrBr',
    linewidth=0,
    legend=False,
)

# ìŠ¤íƒ€ì¼ ì„¤ì •
plt.title("ì‹œêµ°êµ¬ë³„ ì˜ˆì¸¡ ì ìˆ˜", fontsize=26, color='Black')
plt.axis('off')
plt.show()

sorting = top20[["ì‹œë„", "ì‹œêµ°êµ¬", "ì½”ë“œ", "ì‚¬ì—…í™•ë¥ "]]

sorting = sorting.reset_index(drop=True)

sorting

sorting.to_csv("ranking.csv")

"""#RandomForest"""

# ğŸ“Œ 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.preprocessing import StandardScaler

# ğŸ“Œ 2. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
df = pd.read_csv("/content/á„á…¬á„Œá…©á†¼á„€á…¡á„€á…©á†¼á„ƒá…¦á„‹á…µá„á…¥.csv")

# ğŸ“Œ 3. Feature, Target ì„¤ì •
feature_cols = [
    "ëŒ€ì‘ëŠ¥ë ¥ ê³„ìˆ˜(ë‹¨ìœ„: ì ìˆ˜)",
    "ë…¸ì¶œë„ ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)",
    "ë¯¼ê°ë„ ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)",
    "ë³´ì¡°ìˆ˜ì› ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)",
    "ì·¨ì•½ì„±(ë‹¨ìœ„: ì ìˆ˜)"
]
X = df[feature_cols]
y = df["ì‚¬ì—…ì—¬ë¶€"].fillna(0).astype(int)  # NaNì€ 0 ì²˜ë¦¬

# ğŸ“Œ 4. ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ğŸ“Œ 5. í›ˆë ¨/ê²€ì¦ ë¶„ë¦¬
X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, stratify=y, test_size=0.2, random_state=42)

# ğŸ“Œ 6. ëª¨ë¸ í•™ìŠµ
rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)

# ğŸ“Œ 7. ì˜ˆì¸¡ ë° í‰ê°€
y_pred = rf.predict(X_val)
y_prob = rf.predict_proba(X_val)[:, 1]

print("ğŸ“Œ ROC AUC:", roc_auc_score(y_val, y_prob))
print("ğŸ“Œ Classification Report:\n", classification_report(y_val, y_pred))

# ğŸ“Œ 8. í˜¼ë™í–‰ë ¬ ì‹œê°í™”
plt.figure(figsize=(4, 3))
sns.heatmap(confusion_matrix(y_val, y_pred), annot=True, fmt='d', cmap='Blues')
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.tight_layout()
plt.show()

# ğŸ“Œ 9. ì „ì²´ ë°ì´í„° ì‚¬ì—… í™•ë¥  ì˜ˆì¸¡
df["ì‚¬ì—…í™•ë¥ "] = rf.predict_proba(X_scaled)[:, 1]

# ğŸ“Œ 10. ì‚¬ì—…ì´ ì•„ì§ ì•ˆ ëœ ê³³ ì¤‘ì—ì„œ í™•ë¥  ë†’ì€ ìƒìœ„ 20ê°œ ì¶”ì¶œ
top20 = df[df["ì‚¬ì—…ì—¬ë¶€"] == 0].sort_values("ì‚¬ì—…í™•ë¥ ", ascending=False).head(20)

# ğŸ“Œ 11. ì‹œê°í™”
plt.figure(figsize=(8, 6))
sns.barplot(data=top20, y="ì‹œêµ°êµ¬", x="ì‚¬ì—…í™•ë¥ ", palette="viridis")
plt.title("ì‚¬ì—… ìœ ë ¥ í›„ë³´ ìƒìœ„ 20ê°œ ì§€ì—­")
plt.xlabel("ì‚¬ì—… í™•ë¥ ")
plt.ylabel("ì‹œêµ°êµ¬")
plt.tight_layout()
plt.show()

# ğŸ“Œ 12. ìƒìœ„ 20ê°œ ì¶œë ¥
top20[["ì‹œë„", "ì‹œêµ°êµ¬", "ì‚¬ì—…í™•ë¥ "]]

"""# Geo ì‹œê°í™”

"""

import geopandas as gpd

df = pd.read_csv("/content/á„‰á…µá„€á…®á†«á„€á…®á„‡á…§á†¯_á„€á…¡á„†á…®á†·á„ƒá…©á„ƒá…¦á„‹á…µá„á…¥.csv", index_col=0)
shp_path = "/content/BND_SIGUNGU_PG.shp"
gdf = gpd.read_file(shp_path)

gdf[gdf["SIGUNGU_NM"] == "ì¤‘êµ¬"]

df = df[['ì‹œë„', 'ì‹œêµ°êµ¬', 'ì½”ë“œ', 'ëŒ€ì‘ëŠ¥ë ¥ ê³„ìˆ˜(ë‹¨ìœ„: ì ìˆ˜)', 'ë…¸ì¶œë„ ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)',
       'ë¯¼ê°ë„ ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)', 'ë³´ì¡°ìˆ˜ì› ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)', 'ì·¨ì•½ì„±(ë‹¨ìœ„: ì ìˆ˜)', 'ì‚¬ì—…ì—¬ë¶€']]

df["ì½”ë“œ"] = df["ì½”ë“œ"].astype(str)

merged = df.merge(gdf, how='left', left_on='ì½”ë“œ', right_on='SIGUNGU_CD')


merged

merged.to_csv("temp.csv")

"""# ë°ì´í„° ê°€ê³µ

"""

for i in data:
    if i[0].isnumeric():
        i.pop(0)

print(data)

for i in data:
    if len(i) > 7:
        print(i)

import pandas as pd

# ì»¬ëŸ¼ëª… ì§€ì •
columns = [
    "í–‰ì •ë™ëª…",
    "ëŒ€ì‘ëŠ¥ë ¥ ê³„ìˆ˜(ë‹¨ìœ„: ì ìˆ˜)", "ë…¸ì¶œë„ ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)", "ë¯¼ê°ë„ ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)",
    "ë³´ì¡°ìˆ˜ì› ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)", "ì·¨ì•½ì„±(ë‹¨ìœ„: ì ìˆ˜)", "ì·¨ì•½ì„± ë“±ê¸‰(ë‹¨ìœ„: ë“±ê¸‰)"
]

# DataFrameìœ¼ë¡œ ë³€í™˜
df = pd.DataFrame(data, columns=columns)

df.head(100)

df["ì·¨ì•½ì„± ë“±ê¸‰(ë‹¨ìœ„: ë“±ê¸‰)"].value_counts(dropna=False)

print(df.loc[df["ì·¨ì•½ì„± ë“±ê¸‰(ë‹¨ìœ„: ë“±ê¸‰)"] == None])

import pandas as pd
import re

def split_address(addr):
    # ì„¸ì¢…ì‹œ
    if addr.startswith("ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ"):
        return pd.Series(["ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ", "", addr[len("ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ"):]])

    # ì‹œë„ ì¶”ì¶œ
    match_sido = re.match(r"^(.*?[ë„ì‹œ])", addr)
    ì‹œë„ = match_sido.group(1) if match_sido else ""
    ë‚˜ë¨¸ì§€ = addr[len(ì‹œë„):]

    # ì „ì²´ê°€ í–‰ì •ë™ í•˜ë‚˜ë¡œ ëë‚  ìˆ˜ ìˆëŠ” ê²½ìš°: 'ë™', 'ë©´', 'ì' ìœ¼ë¡œ ëë‚˜ë©´
    match_dong = re.match(r"^(.*[ì‹œêµ°êµ¬])(.+[ë™ìë©´])$", ë‚˜ë¨¸ì§€)
    if match_dong:
        ì‹œêµ°êµ¬ = match_dong.group(1)
        í–‰ì •ë™ = match_dong.group(2)
    else:
        # ì¼ë°˜ì ì¸ ê²½ìš° fallback
        match_sigungu = re.match(r"^(.*(ì‹œ|êµ°|êµ¬))", ë‚˜ë¨¸ì§€)
        ì‹œêµ°êµ¬ = match_sigungu.group(1) if match_sigungu else ""
        í–‰ì •ë™ = ë‚˜ë¨¸ì§€[len(ì‹œêµ°êµ¬):]

    return pd.Series([ì‹œë„, ì‹œêµ°êµ¬, í–‰ì •ë™])

df[["ì‹œë„", "ì‹œêµ°êµ¬", "í–‰ì •ë™"]] = df["í–‰ì •ë™ëª…"].apply(split_address)

# ê²°ê³¼ í™•ì¸
print(df)

df.head(20)

df = df[["ì‹œë„", "ì‹œêµ°êµ¬", "í–‰ì •ë™",
         "ëŒ€ì‘ëŠ¥ë ¥ ê³„ìˆ˜(ë‹¨ìœ„: ì ìˆ˜)", "ë…¸ì¶œë„ ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)", "ë¯¼ê°ë„ ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)",
         "ë³´ì¡°ìˆ˜ì› ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)", "ì·¨ì•½ì„±(ë‹¨ìœ„: ì ìˆ˜)", "ì·¨ì•½ì„± ë“±ê¸‰(ë‹¨ìœ„: ë“±ê¸‰)"]]

df.head(500)

df.to_csv("ë°ì´í„°.csv")

cols_to_float = [
    "ëŒ€ì‘ëŠ¥ë ¥ ê³„ìˆ˜(ë‹¨ìœ„: ì ìˆ˜)",
    "ë…¸ì¶œë„ ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)",
    "ë¯¼ê°ë„ ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)",
    "ë³´ì¡°ìˆ˜ì› ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)",
    "ì·¨ì•½ì„±(ë‹¨ìœ„: ì ìˆ˜)"
]
for col in cols_to_float:
    df.loc[:, col] = pd.to_numeric(df[col], errors="coerce")

df

# ì¡°ê±´: ì‹œêµ°êµ¬ê°€ 'ì‹œêµ¬'ë¡œ ëë‚˜ëŠ” ê²½ìš°
mask = df['ì‹œêµ°êµ¬'].str.endswith('ê°•ë‚¨êµ¬ì••êµ¬')

# ì‹œêµ°êµ¬ì—ì„œ 'êµ¬'ë¥¼ ë–¼ê³ , í–‰ì •ë™ ì•ì— ë¶™ì´ê¸°
df.loc[mask, 'í–‰ì •ë™'] = 'ì••êµ¬' + df.loc[mask, 'í–‰ì •ë™']
df.loc[mask, 'ì‹œêµ°êµ¬'] = df.loc[mask, 'ì‹œêµ°êµ¬'].str[:-2]  # 'ì‹œêµ¬' -> 'ì‹œ'

df.loc[df["ì‹œêµ°êµ¬"] == "ê°•ë‚¨êµ¬"]

# ì¡°ê±´: ì‹œêµ°êµ¬ê°€ 'ì‹œêµ¬'ë¡œ ëë‚˜ëŠ” ê²½ìš°
mask = df['ì‹œêµ°êµ¬'].str.endswith('ë‚¨ë™êµ¬êµ¬')

# ì‹œêµ°êµ¬ì—ì„œ 'êµ¬'ë¥¼ ë–¼ê³ , í–‰ì •ë™ ì•ì— ë¶™ì´ê¸°
df.loc[mask, 'í–‰ì •ë™'] = 'êµ¬' + df.loc[mask, 'í–‰ì •ë™']
df.loc[mask, 'ì‹œêµ°êµ¬'] = df.loc[mask, 'ì‹œêµ°êµ¬'].str[:-1]  # 'ì‹œêµ¬' -> 'ì‹œ'

df.loc[df["ì‹œêµ°êµ¬"] == "ë‚¨ë™êµ¬"]

df = df.groupby(["ì‹œë„", "ì‹œêµ°êµ¬"])[["ëŒ€ì‘ëŠ¥ë ¥ ê³„ìˆ˜(ë‹¨ìœ„: ì ìˆ˜)", "ë…¸ì¶œë„ ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)", "ë¯¼ê°ë„ ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)","ë³´ì¡°ìˆ˜ì› ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)", "ì·¨ì•½ì„±(ë‹¨ìœ„: ì ìˆ˜)"]].mean().reset_index()

df

df.to_csv("ì œì£¼ë„.csv")

seoul = df.iloc[:425,:]
incheon = df.iloc[425:580,:]
gyeonggi = df.iloc[580:,:]

seoul = seoul.groupby(["ì‹œë„", "ì‹œêµ°êµ¬"])[["ëŒ€ì‘ëŠ¥ë ¥ ê³„ìˆ˜(ë‹¨ìœ„: ì ìˆ˜)", "ë…¸ì¶œë„ ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)", "ë¯¼ê°ë„ ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)","ë³´ì¡°ìˆ˜ì› ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)", "ì·¨ì•½ì„±(ë‹¨ìœ„: ì ìˆ˜)"]].mean().reset_index()

seoul

incheon = incheon.groupby(["ì‹œë„", "ì‹œêµ°êµ¬"])[["ëŒ€ì‘ëŠ¥ë ¥ ê³„ìˆ˜(ë‹¨ìœ„: ì ìˆ˜)", "ë…¸ì¶œë„ ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)", "ë¯¼ê°ë„ ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)","ë³´ì¡°ìˆ˜ì› ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)", "ì·¨ì•½ì„±(ë‹¨ìœ„: ì ìˆ˜)"]].mean().reset_index()

incheon

gyeongi = gyeonggi.groupby(["ì‹œë„", "ì‹œêµ°êµ¬"])[["ëŒ€ì‘ëŠ¥ë ¥ ê³„ìˆ˜(ë‹¨ìœ„: ì ìˆ˜)", "ë…¸ì¶œë„ ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)", "ë¯¼ê°ë„ ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)","ë³´ì¡°ìˆ˜ì› ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)", "ì·¨ì•½ì„±(ë‹¨ìœ„: ì ìˆ˜)"]].mean().reset_index()

# ì¡°ê±´: ì‹œêµ°êµ¬ê°€ 'ì‹œêµ¬'ë¡œ ëë‚˜ëŠ” ê²½ìš°
mask = df['ì‹œêµ°êµ¬'].str.endswith('í¬ì²œì‹œêµ°')

# ì‹œêµ°êµ¬ì—ì„œ 'êµ¬'ë¥¼ ë–¼ê³ , í–‰ì •ë™ ì•ì— ë¶™ì´ê¸°
df.loc[mask, 'í–‰ì •ë™'] = 'êµ°' + df.loc[mask, 'í–‰ì •ë™']
df.loc[mask, 'ì‹œêµ°êµ¬'] = df.loc[mask, 'ì‹œêµ°êµ¬'].str[:-1]  # 'ì‹œêµ¬' -> 'ì‹œ'

gyeongi

gyeongbuk = gyeongbuk.groupby(["ì‹œë„", "ì‹œêµ°êµ¬"])[["ëŒ€ì‘ëŠ¥ë ¥ ê³„ìˆ˜(ë‹¨ìœ„: ì ìˆ˜)", "ë…¸ì¶œë„ ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)", "ë¯¼ê°ë„ ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)","ë³´ì¡°ìˆ˜ì› ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)", "ì·¨ì•½ì„±(ë‹¨ìœ„: ì ìˆ˜)"]].mean().reset_index()

gyeongbuk

gyeongnam = gyeongnam.groupby(["ì‹œë„", "ì‹œêµ°êµ¬"])[["ëŒ€ì‘ëŠ¥ë ¥ ê³„ìˆ˜(ë‹¨ìœ„: ì ìˆ˜)", "ë…¸ì¶œë„ ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)", "ë¯¼ê°ë„ ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)","ë³´ì¡°ìˆ˜ì› ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)", "ì·¨ì•½ì„±(ë‹¨ìœ„: ì ìˆ˜)"]].mean().reset_index()

gyeongnam

df_all = pd.concat([seoul, incheon, gyeongi], axis=0, ignore_index=True)

df_all

df_all = pd.concat([seoul, incheon, gyeongi], axis=0, ignore_index=True)

df_all.to_csv("ìˆ˜ë„ê¶Œ.csv")

sudo = pd.read_csv("/content/ìˆ˜ë„ê¶Œ.csv")
gangwon = pd.read_csv("/content/á„€á…¡á†¼á„‹á…¯á†«á„ƒá…©.csv")
gyeongsang = pd.read_csv("/content/ê²½ìƒê¶Œ.csv")
chungcheong = pd.read_csv("/content/á„á…®á†¼á„á…¥á†¼á„€á…¯á†«_á„€á…¡á„†á…®á†·á„ƒá…©.csv")
junla = pd.read_csv("/content/ì „ë¼ê¶Œ.csv")
jeju = pd.read_csv("/content/á„Œá…¦á„Œá…®á„ƒá…©.csv")

df_all = pd.concat([sudo, gangwon, gyeongsang, chungcheong, junla, jeju], axis=0, ignore_index=True)

df_all.head(10)

columns = ["ì‹œë„", "ì‹œêµ°êµ¬","ëŒ€ì‘ëŠ¥ë ¥ ê³„ìˆ˜(ë‹¨ìœ„: ì ìˆ˜)",	"ë…¸ì¶œë„ ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)",
           "ë¯¼ê°ë„ ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)",	"ë³´ì¡°ìˆ˜ì› ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)",	"ì·¨ì•½ì„±(ë‹¨ìœ„: ì ìˆ˜)"]

df = df_all[columns]

3 > 2

df.head(10)

df.to_csv("total.csv")

# ì¡°ê±´: ì‹œêµ°êµ¬ê°€ 'ì‹œêµ¬'ë¡œ ëë‚˜ëŠ” ê²½ìš°
mask = junla['ì‹œêµ°êµ¬'].str.endswith('ì§„ë„êµ°êµ°')

# ì‹œêµ°êµ¬ì—ì„œ 'êµ¬'ë¥¼ ë–¼ê³ , í–‰ì •ë™ ì•ì— ë¶™ì´ê¸°
junla.loc[mask, 'ì‹œêµ°êµ¬'] = junla.loc[mask, 'ì‹œêµ°êµ¬'].str[:-1]  # 'ì‹œêµ¬' -> 'ì‹œ'

junla.loc[junla["ì‹œë„"] == "ì „ë¼ë‚¨ë„"]

junla = junla.groupby(["ì‹œë„", "ì‹œêµ°êµ¬"])[["ëŒ€ì‘ëŠ¥ë ¥ ê³„ìˆ˜(ë‹¨ìœ„: ì ìˆ˜)", "ë…¸ì¶œë„ ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)", "ë¯¼ê°ë„ ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)","ë³´ì¡°ìˆ˜ì› ê³„ìˆ˜(ë‹¨ìœ„: ë¬´ì°¨ì›)", "ì·¨ì•½ì„±(ë‹¨ìœ„: ì ìˆ˜)"]].mean().reset_index()

junla

df_all.to_csv("total.csv")

"""# ì·¨í•©"""

import pandas as pd

"""# ê°€ë­„ ë§ˆì´ë‹"""

gamum = pd.read_csv("/content/á„€á…¡á„†á…®á†·á„ƒá…©_á„…á…¢á†¼á„á…µá†¼.csv").iloc[:,1:]
gamum.head(10)

gamum["ê°€ë­„ë“±ìˆ˜"] = range(1, len(gamum) + 1)

gamum

gamum = gamum[["ì‹œë„", "ì‹œêµ°êµ¬", "ê°€ë­„ë“±ìˆ˜"]]
gamum

"""# ìˆ˜ë¦¬ ë§ˆì´ë‹"""

sangsu = pd.read_csv("/content/á„‰á…®á„…á…µá„‰á…¡á†¼á„‰á…®_á„…á…¢á†¼á„á…µá†¼.csv")
sangsu.head(10)

sangsu = sangsu[sangsu["SIGUNGU_NM"].notna()].reset_index(drop=True)
sangsu.columns = ["ìƒìˆ˜ë“±ìˆ˜", "ì‹œë„", "ì‹œêµ°êµ¬", "í™•ë¥ "]
sangsu["ìƒìˆ˜ë“±ìˆ˜"] = range(1, len(sangsu) + 1)

sangsu

sangsu = sangsu[["ì‹œë„", "ì‹œêµ°êµ¬", "ìƒìˆ˜ë“±ìˆ˜"]]

sangsu = sangsu[:148]
sangsu

"""# ë‘ ê°œ ë¨¼ì € merge"""

two = sangsu.merge(gamum, on=["ì‹œë„", "ì‹œêµ°êµ¬"], how="left")
two

two.to_csv("temp.csv")

"""# ìˆ˜ì§ˆ"""

sugil = pd.read_csv("/content/á„‰á…®á„Œá…µá†¯_á„…á…¢á†¼á„á…µá†¼.csv")

sugil

sugil["ìˆ˜ì§ˆë“±ìˆ˜"] = range(1, len(sugil) + 1)

sugil

sugil = sugil[["ì‹œë„", "ì‹œêµ°êµ¬", "ìˆ˜ì§ˆë“±ìˆ˜"]]
sugil

sugil[sugil["ì‹œë„"] == "ì œì£¼ë„"]

three = two.merge(sugil, on=["ì‹œë„", "ì‹œêµ°êµ¬"], how="left")
three

three.to_csv("ë­í‚¹.csv")

ranking = pd.read_csv("//content/á„…á…¢á†¼á„á…µá†¼.csv", index_col=0)

ranking

ranking = ranking[ranking["ì‹œêµ°êµ¬"].notna()].reset_index(drop=True)

ranking

ranking["ìµœì¢…ë“±ìˆ˜"] = (ranking["ìƒìˆ˜ë“±ìˆ˜"] + ranking["ê°€ë­„ë“±ìˆ˜"] + ranking["ìˆ˜ì§ˆë“±ìˆ˜"])/3
ranking

ranking = ranking.sort_values(by="ìµœì¢…ë“±ìˆ˜", ascending=True)
ranking

ranking["ìµœì¢…ë“±ìˆ˜"] = range(1, len(ranking) + 1)
ranking

ranking.to_csv("ìµœì¢…ë­í‚¹.csv")

